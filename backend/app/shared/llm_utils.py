"""
Shared LLM utilities
Low-level utility functions for LLM API calls
"""

import logging
from typing import Dict, List, Optional, Any

logger = logging.getLogger(__name__)


def build_prompt(
    system_prompt: Optional[str] = None,
    user_prompt: Optional[str] = None,
    context: Optional[Dict[str, Any]] = None,
    examples: Optional[List[Dict[str, str]]] = None
) -> List[Dict[str, str]]:
    """
    Build LLM message list

    Args:
        system_prompt: System prompt
        user_prompt: User prompt
        context: Context information (optional)
        examples: Example conversations (optional)

    Returns:
        Formatted message list that can be directly passed to LLM API
    """
    messages = []

    # System prompt
    if system_prompt:
        messages.append({
            "role": "system",
            "content": system_prompt
        })

    # Add context information if available
    if context:
        context_text = _format_context(context)
        if context_text:
            messages.append({
                "role": "system",
                "content": f"[CONTEXT]\n{context_text}\n[/CONTEXT]"
            })

    # Example conversations
    if examples:
        for example in examples:
            if "role" in example and "content" in example:
                messages.append(example)

    # User prompt
    if user_prompt:
        messages.append({
            "role": "user",
            "content": user_prompt
        })

    return messages


def _format_context(context: Dict[str, Any]) -> str:
    """Format context information as text"""
    parts = []
    for key, value in context.items():
        if isinstance(value, (str, int, float, bool)):
            parts.append(f"{key}: {value}")
        elif isinstance(value, dict):
            parts.append(f"{key}: {_format_dict(value)}")
        elif isinstance(value, list):
            parts.append(f"{key}: {', '.join(str(v) for v in value)}")
    return "\n".join(parts)


def _format_dict(d: Dict[str, Any], indent: int = 0) -> str:
    """Recursively format dictionary"""
    parts = []
    prefix = "  " * indent
    for key, value in d.items():
        if isinstance(value, dict):
            parts.append(f"{prefix}{key}:")
            parts.append(_format_dict(value, indent + 1))
        else:
            parts.append(f"{prefix}{key}: {value}")
    return "\n".join(parts)


async def call_llm(
    messages: List[Dict[str, str]],
    llm_provider: Any,
    model: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: Optional[int] = None
) -> Dict[str, Any]:
    """
    Call LLM API

    Args:
        messages: Message list (generated by build_prompt)
        llm_provider: LLM provider object (LLMProvider or LLMProviderManager)
        model: Model name (optional)
        temperature: Temperature parameter
        max_tokens: Maximum token count (optional)

    Returns:
        Dict containing:
            - text: LLM response text
            - usage: Token usage information (if available)
    """
    try:
        # If llm_provider is LLMProviderManager, get the actual provider based on model
        if hasattr(llm_provider, 'get_provider'):
            # Determine provider name from model name
            provider_name = None
            if model:
                if "gemini" in model.lower():
                    provider_name = "vertex-ai"
                elif model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
                    provider_name = "openai"
                elif model.startswith("claude"):
                    provider_name = "anthropic"

            # Get provider from user settings (no fallback)
            from backend.app.shared.llm_provider_helper import get_llm_provider_from_settings
            try:
                provider = get_llm_provider_from_settings(llm_provider)
            except ValueError as e:
                if provider_name:
                    logger.warning(f"Provider {provider_name} not available for model {model}: {e}")
                else:
                    logger.warning(f"LLM provider not available: {e}")
                raise Exception(f"No LLM provider available: {e}")
        else:
            provider = llm_provider

        if not provider:
            raise Exception("No LLM provider available")

        # Debug: log provider type
        logger.info(f"LLM provider type: {type(provider)}, has chat_completion: {hasattr(provider, 'chat_completion')}")

        # Call provider's chat_completion
        # Note: This assumes the provider has a chat_completion method
        # May need to adjust based on different providers
        if hasattr(provider, 'chat_completion'):
            # Check if provider supports additional parameters
            import inspect
            sig = inspect.signature(provider.chat_completion)
            params = {}
            if 'temperature' in sig.parameters:
                params['temperature'] = temperature

            if not model:
                import os
                from backend.app.services.conversation.model_context_presets import get_model_name_from_env
                model = get_model_name_from_env() or "gpt-4o-mini"

            model_name = model
            is_newer_model = model_name and ("gpt-5" in model_name or "o1" in model_name.lower() or "o3" in model_name.lower())

            if is_newer_model:
                # Newer models support higher token limits
                default_token_limit = max_tokens or 8000
                if 'max_completion_tokens' in sig.parameters:
                    params['max_completion_tokens'] = default_token_limit
            else:
                # Older models (gpt-3.5-turbo, gpt-4o-mini) have lower limits, cap at 4096
                default_token_limit = min(max_tokens or 4096, 4096)
                if 'max_tokens' in sig.parameters:
                    params['max_tokens'] = default_token_limit

            response_text = await provider.chat_completion(
                messages=messages,
                model=model_name,
                **params
            )

            # Try to get usage information (if provider provides it)
            usage = {}
            if hasattr(provider, 'last_usage'):
                usage = provider.last_usage

            return {
                "text": response_text,
                "usage": usage
            }
        else:
            error_msg = f"Provider {type(provider)} does not have chat_completion method. Provider: {provider}"
            logger.error(error_msg)
            raise Exception(error_msg)

    except Exception as e:
        logger.error(f"LLM call failed: {e}")
        raise


def extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
    """
    Extract JSON object from text

    Args:
        text: Text containing JSON

    Returns:
        Extracted JSON object, or None if failed
    """
    import json
    import re

    # Try to parse the entire text directly
    try:
        return json.loads(text)
    except:
        pass

    # Try to extract JSON object (prefer larger/more complete matches)
    json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
    matches = re.findall(json_pattern, text, re.DOTALL)

    # Sort by length (descending) to prefer larger/more complete JSON objects
    matches = sorted(matches, key=len, reverse=True)

    for match in matches:
        try:
            parsed = json.loads(match)
            # Prefer objects with "tasks" key if it exists
            if isinstance(parsed, dict) and "tasks" in parsed:
                return parsed
        except:
            continue

    # If no match with "tasks" key, try all matches
    for match in matches:
        try:
            return json.loads(match)
        except:
            continue

    return None
