"""
Shared LLM utilities
Low-level utility functions for LLM API calls
"""

import logging
from typing import Dict, List, Optional, Any

logger = logging.getLogger(__name__)


def build_prompt(
    system_prompt: Optional[str] = None,
    user_prompt: Optional[str] = None,
    context: Optional[Dict[str, Any]] = None,
    examples: Optional[List[Dict[str, str]]] = None
) -> List[Dict[str, str]]:
    """
    Build LLM message list

    Args:
        system_prompt: System prompt
        user_prompt: User prompt
        context: Context information (optional)
        examples: Example conversations (optional)

    Returns:
        Formatted message list that can be directly passed to LLM API
    """
    messages = []

    # System prompt
    if system_prompt:
        messages.append({
            "role": "system",
            "content": system_prompt
        })

    # Add context information if available
    if context:
        context_text = _format_context(context)
        if context_text:
            messages.append({
                "role": "system",
                "content": f"[CONTEXT]\n{context_text}\n[/CONTEXT]"
            })

    # Example conversations
    if examples:
        for example in examples:
            if "role" in example and "content" in example:
                messages.append(example)

    # User prompt
    if user_prompt:
        messages.append({
            "role": "user",
            "content": user_prompt
        })

    return messages


def _format_context(context: Dict[str, Any]) -> str:
    """Format context information as text"""
    parts = []
    for key, value in context.items():
        if isinstance(value, (str, int, float, bool)):
            parts.append(f"{key}: {value}")
        elif isinstance(value, dict):
            parts.append(f"{key}: {_format_dict(value)}")
        elif isinstance(value, list):
            parts.append(f"{key}: {', '.join(str(v) for v in value)}")
    return "\n".join(parts)


def _format_dict(d: Dict[str, Any], indent: int = 0) -> str:
    """Recursively format dictionary"""
    parts = []
    prefix = "  " * indent
    for key, value in d.items():
        if isinstance(value, dict):
            parts.append(f"{prefix}{key}:")
            parts.append(_format_dict(value, indent + 1))
        else:
            parts.append(f"{prefix}{key}: {value}")
    return "\n".join(parts)


async def call_llm(
    messages: List[Dict[str, str]],
    llm_provider: Any,
    model: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: Optional[int] = None
) -> Dict[str, Any]:
    """
    Call LLM API

    Args:
        messages: Message list (generated by build_prompt)
        llm_provider: LLM provider object (LLMProvider or LLMProviderManager)
        model: Model name (optional)
        temperature: Temperature parameter
        max_tokens: Maximum token count (optional)

    Returns:
        Dict containing:
            - text: LLM response text
            - usage: Token usage information (if available)
    """
    try:
        # If llm_provider is LLMProviderManager, get the actual provider based on model
        if hasattr(llm_provider, 'get_provider'):
            # Determine provider name from model name
            provider_name = None
            if model:
                if "gemini" in model.lower():
                    provider_name = "vertex-ai"
                elif model.startswith("gpt") or model.startswith("o1") or model.startswith("o3"):
                    provider_name = "openai"
                elif model.startswith("claude"):
                    provider_name = "anthropic"

            # Get provider from user settings (no fallback)
            from backend.app.shared.llm_provider_helper import get_llm_provider_from_settings
            try:
                provider = get_llm_provider_from_settings(llm_provider)
            except ValueError as e:
                if provider_name:
                    logger.warning(f"Provider {provider_name} not available for model {model}: {e}")
                else:
                    logger.warning(f"LLM provider not available: {e}")
                raise Exception(f"No LLM provider available: {e}")
        else:
            provider = llm_provider

        if not provider:
            raise Exception("No LLM provider available")

        # Debug: log provider type
        logger.info(f"LLM provider type: {type(provider)}, has chat_completion: {hasattr(provider, 'chat_completion')}")

        # Call provider's chat_completion
        # Note: This assumes the provider has a chat_completion method
        # May need to adjust based on different providers
        if hasattr(provider, 'chat_completion'):
            # Check if provider supports additional parameters
            import inspect
            sig = inspect.signature(provider.chat_completion)
            params = {}
            if 'temperature' in sig.parameters:
                params['temperature'] = temperature

            if not model:
                import os
                from backend.app.services.conversation.model_context_presets import get_model_name_from_env
                model = get_model_name_from_env() or "gpt-4o-mini"

            model_name = model
            is_newer_model = model_name and ("gpt-5" in model_name or "o1" in model_name.lower() or "o3" in model_name.lower())
            is_gemini_model = model_name and "gemini" in model_name.lower()

            if is_newer_model:
                # Newer models support higher token limits
                default_token_limit = max_tokens or 8000
                if 'max_completion_tokens' in sig.parameters:
                    params['max_completion_tokens'] = default_token_limit
            elif is_gemini_model:
                # Gemini models support up to 8192 output tokens via max_completion_tokens
                default_token_limit = max_tokens or 8192
                if 'max_completion_tokens' in sig.parameters:
                    params['max_completion_tokens'] = default_token_limit
                elif 'max_tokens' in sig.parameters:
                    params['max_tokens'] = default_token_limit
            else:
                # Older models (gpt-3.5-turbo, gpt-4o-mini) have lower limits, cap at 4096
                default_token_limit = min(max_tokens or 4096, 4096)
                if 'max_tokens' in sig.parameters:
                    params['max_tokens'] = default_token_limit

            response_text = await provider.chat_completion(
                messages=messages,
                model=model_name,
                **params
            )

            # Try to get usage information (if provider provides it)
            usage = {}
            if hasattr(provider, 'last_usage'):
                usage = provider.last_usage

            return {
                "text": response_text,
                "usage": usage
            }
        else:
            error_msg = f"Provider {type(provider)} does not have chat_completion method. Provider: {provider}"
            logger.error(error_msg)
            raise Exception(error_msg)

    except Exception as e:
        logger.error(f"LLM call failed: {e}")
        raise


def extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
    """
    Extract JSON object from text, handling truncated JSON from MAX_TOKENS.

    Args:
        text: Text containing JSON (may be truncated)

    Returns:
        Extracted JSON object, or None if failed
    """
    import json
    import re

    def _find_balanced_json(candidate_text: str) -> Optional[str]:
        """Return first balanced JSON object substring (handles deeply nested braces)."""
        start = candidate_text.find("{")
        while start != -1:
            depth = 0
            in_string = False
            escape = False
            for i in range(start, len(candidate_text)):
                ch = candidate_text[i]
                if in_string:
                    if escape:
                        escape = False
                    elif ch == "\\":
                        escape = True
                    elif ch == '"':
                        in_string = False
                else:
                    if ch == '"':
                        in_string = True
                    elif ch == "{":
                        depth += 1
                    elif ch == "}":
                        depth -= 1
                        if depth == 0:
                            return candidate_text[start:i + 1]
            start = candidate_text.find("{", start + 1)
        return None

    # Try to parse the entire text directly
    try:
        return json.loads(text)
    except:
        pass

    # Remove markdown code blocks if present
    text_clean = text.strip()
    if text_clean.startswith("```json"):
        text_clean = text_clean[7:]
    if text_clean.startswith("```"):
        text_clean = text_clean[3:]
    if text_clean.endswith("```"):
        text_clean = text_clean[:-3]
    text_clean = text_clean.strip()

    # Try to parse cleaned text
    try:
        return json.loads(text_clean)
    except:
        pass

    # Try to locate a balanced JSON object even with nested structures
    balanced_json = _find_balanced_json(text_clean)
    if balanced_json:
        try:
            return json.loads(balanced_json)
        except:
            pass

    # Try to extract JSON object (prefer larger/more complete matches)
    json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
    matches = re.findall(json_pattern, text_clean, re.DOTALL)

    # Sort by length (descending) to prefer larger/more complete JSON objects
    matches = sorted(matches, key=len, reverse=True)

    for match in matches:
        try:
            parsed = json.loads(match)
            # Prefer objects with "tasks" key if it exists
            if isinstance(parsed, dict) and "tasks" in parsed:
                return parsed
        except:
            continue

    # If no match with "tasks" key, try all matches
    for match in matches:
        try:
            return json.loads(match)
        except:
            continue

    # Last resort: try to extract partial JSON from truncated response
    # This handles cases where JSON is cut off mid-structure (e.g., MAX_TOKENS)
    if text_clean.startswith("{"):
        try:
            # Try to extract completed key-value pairs even if JSON is truncated
            # Look for patterns like "key": "value" or "key": value
            partial_json = {}

            # Find all completed key-value pairs before truncation
            # Pattern: "key": (value|"string")
            import re
            kv_pattern = r'"([^"]+)":\s*("(?:[^"\\]|\\.)*"|[^,}\]]+?)(?=,\s*"|,|\s*})'
            matches = re.finditer(kv_pattern, text_clean, re.DOTALL)

            for match in matches:
                key = match.group(1)
                value_str = match.group(2).strip()

                # Try to parse the value
                try:
                    # If it's a quoted string, unquote it
                    if value_str.startswith('"') and value_str.endswith('"'):
                        value = json.loads(value_str)  # This handles escaped quotes
                    # If it's a number, parse it
                    elif value_str.replace('.', '').replace('-', '').isdigit():
                        value = float(value_str) if '.' in value_str else int(value_str)
                    # If it's boolean or null
                    elif value_str in ['true', 'false', 'null']:
                        value = json.loads(value_str)
                    # If it's an array or object, try to parse it
                    elif value_str.startswith('[') or value_str.startswith('{'):
                        try:
                            value = json.loads(value_str)
                        except:
                            continue  # Skip incomplete arrays/objects
                    else:
                        continue  # Skip unparseable values

                    partial_json[key] = value
                except:
                    continue

            # If we extracted at least lens_id, return partial JSON
            if "lens_id" in partial_json and len(partial_json) > 0:
                logger.warning(f"Extracted partial JSON from truncated response: {list(partial_json.keys())}")
                return partial_json
        except Exception as e:
            logger.debug(f"Failed to extract partial JSON: {e}")

    return None
